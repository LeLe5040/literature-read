识别 用户贬损倾向，包括：
二分类（推荐 vs 贬损）；
三分类（推荐 / 中立 / 贬损）；
支持 跨期验证；
支持 多模型融合：主观模型、客观模型 → 博弈融合；
支持 特征选择 + 贝叶斯优化 + 自训练机制。


任务不仅仅是建一个贬损识别模型，而是设计一整套分层建模 + 
主客观分治 + 自训练增强 + 博弈融合的系统，解决实际中的标注不完整、用户主观异议、跨期泛化等复杂问题。
| 模块    | 内容                                               | 用途                               |
| ----- | ------------------------------------------------ | -------------------------------- |
| 特征准备  | `get_features()`                                 | 获取主客观数据、按CEI过滤、拼接KQI/TRAJ/POOR特征 |
| 标签处理  | `transform_value_2 / 3 / route_label_transform`  | 支持二分类 / 三分类 / 路由式标签              |
| 特征选择  | `VarianceThreshold + LightGBM + SelectFromModel` | 筛选重要特征（控制 max\_features）         |
| 模型调参  | `Optuna 贝叶斯优化`                                   | 自动选择最优超参（AUC或macro-AUC）          |
| 自训练机制 | `置信度伪标签 + 多轮迭代`                                  | 引入未标注样本，提升模型鲁棒性                  |
| 模型评估  | `evaluate_index + model_evaluation`              | 输出 accuracy / f1 / AUC 等指标       |
| 跨期验证  | `cross_period_verification()`                    | 在测试数据上验证泛化能力                     |
| 主客观划分 | `route_user()`                                   | 通过label或模型输出区分主观/客观用户            |
| 模型融合  | `game_strategy_verification()`                   | 主客观模型结果融合（包括概率校准 + 动态博弈）         |

流程
| 维度    | 实现                     |
| ----- | ---------------------- |
| 模型类型  | 二分类 / 三分类 LightGBM     |
| 特征处理  | 方差过滤 + Boost重要性        |
| 参数优化  | Optuna 贝叶斯优化           |
| 半监督学习 | 自训练（pseudo-label）      |
| 多模型   | 主观 / 客观分治              |
| 模型融合  | 置信度博弈（置信度 + 校准 + 分布分析） |
| 跨期泛化  | 支持不同时间/城市间泛化验证         |

1、特征准备
【数据融合】  从三大类数据库中抽取特征：
① KQI：用户网络质量指标；
② POOR_EVENT：弱覆盖、掉线、卡顿等事件特征；
③ TRAJECTORY：用户轨迹行为、空间活跃特征。

特征合并 相同的小区标签 中默认的z-score进行标签化
多源数据融合 ：用户的主观评分受多因素影响，单一维度（如KQI）不能反映用户对服务的整体感受
Q: 你们有没有考虑特征选择提前做降维？
A: 有，我们会在特征选择模块做特征筛选，前面尽量保留原始信息，避免提前压缩
Q: TRAJECTORY 特征  用户行为相关的特征数据，用户位置数据 个人属性数据  离家距离熵
。。。。。。


2、标签处理（transform_value + route_label_transform）

通过规则映射，将原始的 NPS 分数（0）与 CEI 指标（10）转化为：
标准二分类标签（推荐 vs 贬损）；
模糊三分类标签（推荐 / 中立 / 贬损）；
认知-体验错配路由标签（主观错配识别）；
高斯评分拟合标签（用于软分类，探索性使用）；

原始 NPS 是离散评分，不便于直接监督学习	将其映射为 2/3 分类，提高可学性
Q: 你这个 route_label 是不是 hard rule 太死板了？容易过拟合？
A：是的，这是我们最开始设计的方式，只能粗略识别出明显主观错配用户。但由于规则的边界很硬（例如 CEI>80 / NPS<7），
对 borderline 样本没有容错，泛化能力较差。所以我们后来采用了自训练（self-training）机制进行标签矫正：先用硬
规则找一小部分高置信度主观错配样本，训练一个二分类模型；再用模型生成其他样本的 soft label 伪标签，逐轮迭代更新。
这样可以让模型自己学习“什么是主观错配”，而不是强制灌标签。实验中我们发现这能提升 2%~3% 的识别准确率，且显著提
高了跨期迁移表现。

我们一开始是用 Hard Rule，比如 CEI 很高但 NPS 低，就打成“主观错配”。但这种方式的边界非常死，容易错判。

所以我们采用了自训练机制：
先把一小部分极端错配样本提出来，训练初始模型；
然后让模型自己去预测其余样本的错配概率；
如果它很自信（比如 p > 0.9），我们就把这些样本也加入训练集；
每轮扩展一点，逐步建立一个更泛化、可靠的“主观错配识别器”。
这样的方式可以自动适配不同地区，不再依赖手工经验划分边界。
最终形成稳定的主客观错配识别模型，识别准确率提高约 3.4%。


3、特征选择
| 方法                                                | 说明                   |
| ----------------------------------------            | -------------------- |
| **方差过滤（VarianceThreshold）**                    | 去掉取值太少、基本恒定的特征       |
| **模型重要性选择（LightGBM Feature Importance）**    | 利用训练后的树模型评分，保留重要特征   |
| **SelectFromModel（SFM）**                           | 封装器方法，根据模型打分自动选择重要特征 |

LightGBM 通常做法：
取前 N 个特征（如 top 100）  或设定 importance_threshold，筛掉低权重特征
split：特征被用来分裂的次数（更稳健）
gain：特征带来的信息增益（更敏感）

| 问题                          | 答法                                                                   |
| --------------------------- | -------------------------------------------------------------------- |
| Q: 你为啥用 `Gain` 而不是 `Split`？ | Gain 更细粒度，能量化每个分裂带来的提升。但我们也做过比对，Split 更稳健，所以会结合使用做 double filter。    |
| Q: SFM 阈值怎么设的？              | 一般设为 `"mean"` 或 `"median"`，也可以用 `percentile` 的方式设置前30%重要性以上的特征。      |
| Q: 如果特征之间有强共线性怎么办？          | 我们会在模型评估阶段结合 SHAP 分析，避免冗余特征“重复计分”，也会适当使用 PCA/聚类分析做冗余压缩。              |
| Q: 为什么不直接用 SHAP 选特征？        | SHAP 的可解释性强但计算慢，不适合我们这种大规模站点多任务建模。LightGBM 本身提供强相关性评估能力，我们优先选择更快的方式。 |


我们的特征选择流程一方面通过方差过滤快速清洗无效特征，另一方面借助主模型 LightGBM 的特征重要性来进行重要性筛选，并通过 SelectFromModel 自动化控制阈值。
在模型性能几乎无损的情况下，成功将特征维度压缩近60%，大幅提升了训练速度与部署效率。

4、模型调参（Optuna 贝叶斯优化）
通过自动超参数优化框架，搜索最优的 LightGBM 模型配置，在 AUC / Macro-F1 等指标上达到最优，同时控制训练时间与模型复杂度。
| 方法             | 内容                                                                                                                                               |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| Optuna         | 基于 Tree-structured Parzen Estimator (TPE) 的贝叶斯超参搜索库                                                                                              |
| LGBMClassifier | 调参目标模型（用于分类任务）                                                                                                                                   |
| 评价指标           | `AUC`, `Macro-F1`, `Logloss` 可切换                                                                                                                 |
| 搜索空间           | `num_leaves`, `max_depth`, `min_child_weight`, `learning_rate`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`, `scale_pos_weight` 等 |

贝叶斯优化基于已尝试参数的历史评估结果，预测下一组更可能带来好结果的参数组合。 我们参数多，用这个最好 
【Optuna 的核心结构】：
使用 TPE (Tree-structured Parzen Estimator) 模型模拟目标函数；
动态调整采样策略，更倾向于试验“有希望的参数区域”；
实现快速收敛 & 高效搜索。

Q: 你调了哪些参数？	包括叶子数、深度、采样率、正则系数、类别不平衡权重等，重点调优 num_leaves, learning_rate, scale_pos_weight 等对 AUC 敏感的参数。
Q: 用 AUC 还是 F1 作为优化指标？	看任务：二分类任务中我们一般用 AUC；三分类或样本不平衡时会加权 Macro-F1。我们实际尝试过两个方向，最后选了 multi-objective 形式。

5、自训练机制（Soft-label + 多轮伪标签）

我们使用了基于 soft-label 的伪标签自训练机制来识别主观错配用户。起初只用极少量的高置信样本，通过 LightGBM 训练出初始分类器；然后让模型自己去预测其余
样本的错配概率。

对于预测置信度很高的样本（如 p>0.9 或 p<0.1），我们就将其作为软标签样本加入下一轮训练。这样逐轮扩展训练集，实现了模型对主观错配样本的自动学习与标注修正。
实验中该机制提升了识别准确率 8%+，且显著增强了跨站泛化能力。

6、模型评估
| 场景                  | 推荐指标                        | 原因                          |
| ------------------- | --------------------------- | --------------------------- |
| 样本均衡（客观贬损识别）        | Accuracy、F1                 | 精度和召回都要                     |
| 主观错配识别（样本不均衡）       | **AUC**、**Macro-F1**、Recall | 不能只靠 Accuracy，正类 Recall 更关键 |
| 多分类（如三类标签：正常、主观、客观） | Macro-F1                    | 各类平均考虑                      |
| 小样本测试集（跨期）          | AUC                         | 不受阈值影响、适合泛化评估               |

| 问题                         | 答法                                                                        |
| -------------------------- | ------------------------------------------------------------------------- |
| Q: 为什么不用 Accuracy 作为评估指标？  | 在主观识别中，正负样本极不均衡，Accuracy 不能体现模型是否真的识别出主观错配。我们更关注 Recall 和 AUC。            |
| Q: Macro-F1 和 Micro-F1 区别？ | Macro-F1 是对各类别 F1 的简单平均，适合不均衡任务；Micro-F1 是对所有样本统一统计，偏向样本多的类。我们用 Macro-F1。 |
| Q: AUC 和 Logloss 区别？       | AUC 是排序指标，不依赖阈值；Logloss 衡量概率输出与真实标签的距离，更细致，但受 outlier 影响大。                |
| Q: 怎么选择最优模型？               | 我们会从 AUC、Macro-F1、Recall 维度综合比较，在业务允许范围内还会关注模型大小、推理速度等部署属性。               |

在模型评估方面，我们不会只看 Accuracy，而是综合使用 AUC、Macro-F1、Recall 来评估模型的排序能力、识别能力和稳定性。尤其在主观错配识别中，样本极度不均衡，
所以我们更重视模型的 AUC 和主观样本的 Recall 表现。

实验中我们通过自训练机制将主观错配 Recall 提升了近 18%，同时保持了整体模型的 F1 和泛化能力，在评估上我们有一套完整的 evaluate_index + 报告结构，
能够快速做跨模型对比。

7、跨期验证
| 问题               | 答法                                                                 |
| ---------------- | ------------------------------------------------------------------ |
| Q: 你怎么验证模型的泛化能力？ | 我们采用了跨期验证策略，包括时间滑窗和站点迁移，在不同时间段或城市上测试模型，查看 AUC 和 Recall 是否明显下降。     |
| Q: 模型跨期效果变差怎么办？  | 可以采用 soft-label 机制补充训练样本，或者引入更稳健的时间不变性特征，比如用户长期行为特征、归一化 CEI/NPS 等。 |
| Q: 是否用交叉验证？      | 常规任务中用 5-fold CV，但对于跨期任务，我们采用的是时序 hold-out 或滑动验证，避免信息泄漏。           |
| Q: 时间序列信息怎么处理？   | 在特征工程阶段我们加入了时间窗口特征、频次衰减因子、周/月周期等，防止模型对时间高度敏感。                      |


在模型评估阶段，我们不仅关注当前数据的训练效果，也非常重视模型的跨期泛化能力。我们采用了 cross-period 验证策略，即在 2024Q1 训练，、
在 2024Q2 或 Q3 上测试，观察 AUC、Recall 的变化趋势。

实验中发现单模型跨期性能下滑明显，因此我们引入了 soft-label 自训练和时间增强特征，提升模型鲁棒性，最终使得主观错配识别在未来数据
上仍维持 72%+ 的召回水平，达到了部署要求。


