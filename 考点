✅ 01. 交叉熵损失函数

对于二分类任务，我们常用的损失函数是交叉熵（Cross Entropy）：

这个损失函数来源于最大似然估计。它对应于假设正类服从 Bernoulli 分布，对似然函数取对数后得到的结果。

| 考点分类           | 子知识点                                  | 面试可能问法                          | 推荐准备内容                                |
| -------------- | ------------------------------------- | ------------------------------- | ------------------------------------- |
| 📌 激活函数        | Sigmoid / Softmax / ReLU              | “你用的是哪种激活函数？为什么？”               | Softmax 多分类用；Sigmoid 二分类用；ReLU 可防梯度消失 |
| 📌 损失函数        | CrossEntropy / LogLoss / Hinge        | “为啥用交叉熵？”“它和最大似然啥关系？”           | 交叉熵就是负对数似然；逻辑回归默认使用                   |
| 📌 梯度更新        | Forward / Backpropagation / LR梯度      | “怎么从交叉熵推导梯度？”                   | 明确公式 + 链式法则推导                         |
| 📌 AUC指标       | ROC / PR-AUC / KS / 两个物理意义            | “AUC 能解释一下吗？”“AUC = 0.5 是什么情况？” | 会画图、能说出排序对、暴力计算法                      |
| 📌 overfitting | Dropout / Early stop / Regularization | “你如何解决过拟合？”                     | Dropout、L2正则、提早终止等                    |
| 📌 自监督伪标签      | Pseudo-label + confidence             | “怎么选伪标签？阈值多少？”                  | P>0.9 / P<0.1；用 soft label 减少错误引导     |
| 📌 回归 vs 分类    | loss区别 + 输出层激活函数                      | “回归任务能不能用交叉熵？”                  | 回归一般用MSE/MAPE，不适合交叉熵                  |

 Q1：你用的是哪种激活函数？为什么？
我们在主客观贬损识别系统中主要使用的是 Sigmoid 函数，因为我们是二分类任务，要输出的是一个属于主观/非主观的概率值。
如果是多分类场景，比如将贬损用户分成三类（主观贬损、客观贬损、非贬损），这时我们就会用 Softmax，它可以输出多个类别的概率。
对于深层神经网络部分，比如我们之前试验过 MLP 模型，就使用了 ReLU，它的优点是不会出现梯度消失问题。

损失函数（Cross Entropy / LogLoss / Hinge）

Q2：为啥用交叉熵？它和最大似然有啥关系？
交叉熵是二分类任务中最常用的损失函数，原因有两个：
第一，它直接衡量的是模型预测的概率与真实标签之间的差异，尤其适合概率输出；
第二，它本质上是最大似然估计的结果，在逻辑回归中，我们假设标签服从伯努利分布，最大似然估计推导出来的 loss 正好就是交叉熵形式。
这比均方误差更适合分类，因为它对错误预测惩罚更强，优化效果更快。

Q5：AUC 能解释一下吗？
AUC 是二分类模型下，ROC 曲线下的面积，表示模型对正负样本排序能力的强弱。
1.随机抽取一个正样本和负样本，预测得分高于负样本的概率；
2.排序正确的正负样本对的比例。

在项目中，我们使用了几种策略来防止过拟合：

Q7：你们如何防止模型过拟合？
第一是 特征选择，我们通过方差过滤和 LightGBM 的特征重要性挑选了最关键的特征，减少维度；
第二是 LightGBM 本身提供了 L2 正则化 和 早停策略；
如果模型换成神经网络，我们还会加上 Dropout 或 BatchNorm；
另外，在自训练模块中，我们用 高置信度阈值控制伪标签引入，避免模型学到错误样本。

Q8：你们怎么选伪标签？阈值怎么设的？
我们采用自训练机制，把模型预测结果中置信度高的未标注样本当成伪标签引入训练。
如果预测概率 P(y=1)>0.9，就认为是正样本；
如果 P(y=1)<0.1，就认为是负样本；
这样做的好处是：只引入模型非常有把握的样本，提升了自训练的稳定性。后期我们还尝试了用 
Soft-label 而不是 Hard-label，这样能更好地表达模型的不确定性，避免错误样本对模型造成误导。

Q10：回归任务常用的 loss 和激活函数有哪些？
| Loss      | 用途            | 输出层激活函数 |
| --------- | ------------- | ------- |
| MSE（均方误差） | 普通回归          | 无需激活函数  |
| MAE（绝对误差） | 对异常值鲁棒        | 无       |
| Huber     | 兼顾 MSE 和 MAE  | 无       |
| MAPE      | 百分比误差，适合比例型预测 | 无       |

❓ Q11：为什么分类任务最后要加激活函数，而回归任务不用？
分类任务需要将模型输出值变成 合法概率分布，所以必须用 Sigmoid 或 Softmax 把结果压缩到 [0,1] 范围。
而回归任务输出的是连续值，比如房价、用户评分，它本身就是连续区间，不需要激活函数处理。


