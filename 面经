
1. BN和dropout训练和测试的差异
Dropout：
训练时：假设保留率是 0.8，每轮都会随机关闭 20% 的神经元。
测试时：不再丢神经元，但要对输出进行缩放（乘 0.8），以匹配训练时的平均激活强度。

Batch Normalization：
训练时：每个小批次算自己的均值/方差，然后归一化。
测试时：用训练过程中记录下来的“全局平均统计量”（running mean/var），使得结果更稳定。

2. 在二分类任务中常用的 loss 函数和最后一层的激活函数分别是什么？为什么
在二分类任务中，常用的损失函数是 Binary Cross Entropy（BCE）或 BCEWithLogitsLoss，最后一层的激活函数通常是 Sigmoid。
如果使用 BCELoss，则最后一层要加 Sigmoid，用于将输出映射到概率空间 (0,1)。
如果使用 BCEWithLogitsLoss，则不加 Sigmoid，因为它内部已集成 sigmoid + BCE，更数值稳定，是工业界推荐做法。


3. word2vec用的哪一个，解释这个方法，损失函数是交叉熵吗，损失函数的公式？损失函数中负样本损失是一个还是多个

4.逻辑回归损失函数级梯度更新的工时
交叉熵
损失函数
二分类神经网络 含前馈和后馈

transformer的结构  里面的attention机制
多头注意力
注意力计算的复杂度  降低复杂度的办法 
transformer 的大致描述一下，讲一下前向流程？
讲一下bert, bert训练的损失函数是什么？
bert 的三种编码向量为什么可以直接相加？
分类模型一般用什么损失？为什么用交叉熵不用MSE？写一下交叉熵的公式？



梯度消失 梯度爆炸 梯度下降  原因 如何解决 
L1L2正则化
防止过拟合的办法
交叉熵和最大似然的关系 
损失函数有哪些  

softmax与二分类有什么特点 
二分类任务（ CTR / CVR )使用的 loss 和最后一层的激活函数是什么？为什么？写出这里的每个激活函数、 loss function
2．为什么二分类任务的预估值等于 label 的均值，前提：做一个二分类任务，输入的数据 x 都相同，其 label 存在不相同。
3．回归任务可以使用哪些 loss function ？最后一层分别用什么激活函数？写出这里的每个 loss function ，并关于预估值求导
随机森林  机器学习 
kmeans  损失函数 每个蔟的中心点事怎么选的 参数等等

手撕 layer normation
手撕大数相加 
手撕最大组数和
手撕 最短无序子数组
code题目:打家劫舍
手撕：计算岛屿数量
4. 手撕：最长递增子序列
滑动窗口的最大值
三数之和 

AUC的含义  两个物理意义 公式和时间复杂度  随机采样负样本对AUC有什么影响
softmax的真正底层原理，问的很细致（答得很浅）
讲一下XGB, 决策树如何选择特征（项目中涉及树模型
你了解哪些多模态模型？模态是怎么做融合的？


论文相关 
数据集介绍 如何构建的
论文的创新点和细节问答
论文的后续工作介绍 优化的点是啥


