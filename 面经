
1. BN和dropout训练和测试的差异
Dropout：
训练时：假设保留率是 0.8，每轮都会随机关闭 20% 的神经元。
测试时：不再丢神经元，但要对输出进行缩放（乘 0.8），以匹配训练时的平均激活强度。

Batch Normalization：
训练时：每个小批次算自己的均值/方差，然后归一化。
测试时：用训练过程中记录下来的“全局平均统计量”（running mean/var），使得结果更稳定。

2. 在二分类任务中常用的 loss 函数和最后一层的激活函数分别是什么？为什么
在二分类任务中，常用的损失函数是 Binary Cross Entropy（BCE）或 BCEWithLogitsLoss，最后一层的激活函数通常是 Sigmoid。
如果使用 BCELoss，则最后一层要加 Sigmoid，用于将输出映射到概率空间 (0,1)。
如果使用 BCEWithLogitsLoss，则不加 Sigmoid，因为它内部已集成 sigmoid + BCE，更数值稳定，是工业界推荐做法。


3. word2vec用的哪一个，解释这个方法，损失函数是交叉熵吗，损失函数的公式？损失函数中负样本损失是一个还是多个

4.逻辑回归损失函数级梯度更新的工时
交叉熵
损失函数
二分类神经网络 含前馈和后馈

transformer的结构  里面的attention机制
多头注意力
注意力计算的复杂度  降低复杂度的办法 
transformer 的大致描述一下，讲一下前向流程？
讲一下bert, bert训练的损失函数是什么？
bert 的三种编码向量为什么可以直接相加？
分类模型一般用什么损失？为什么用交叉熵不用MSE？写一下交叉熵的公式？



梯度消失 梯度爆炸 梯度下降  原因 如何解决 
L1L2正则化
防止过拟合的办法
交叉熵和最大似然的关系 
损失函数有哪些  
常见的激活函数及优缺点 
各种归一化对的方法（layernorm，batchnorm）的却别
解释过拟合和欠拟合，如何结果的


softmax与二分类有什么特点 
二分类任务（ CTR / CVR )使用的 loss 和最后一层的激活函数是什么？为什么？写出这里的每个激活函数、 loss function
2．为什么二分类任务的预估值等于 label 的均值，前提：做一个二分类任务，输入的数据 x 都相同，其 label 存在不相同。
3．回归任务可以使用哪些 loss function ？最后一层分别用什么激活函数？写出这里的每个 loss function ，并关于预估值求导
随机森林  机器学习 
kmeans  损失函数 每个蔟的中心点事怎么选的 参数等等

手撕 layer normation
手撕大数相加 
手撕最大组数和
手撕 最短无序子数组
code题目:打家劫舍
手撕：计算岛屿数量
4. 手撕：最长递增子序列
滑动窗口的最大值
三数之和 

AUC的含义  两个物理意义 公式和时间复杂度  随机采样负样本对AUC有什么影响
softmax的真正底层原理，问的很细致（答得很浅）
讲一下XGB, 决策树如何选择特征（项目中涉及树模型
你了解哪些多模态模型？模态是怎么做融合的？


论文相关 
数据集介绍 如何构建的
论文的创新点和细节问答
论文的后续工作介绍 优化的点是啥
、
在我们的验证集上，Cross-Attention 和 多尺度窗口带来最明显收益：前者直接加强‘质量→体验’的对齐，显著提升了贬损召回；后者让模型避免只盯短期波动。独立 Encoder带来更稳定的泛化，时段/地域切换时鲁棒性更好。去掉归因 Loss后，主观样本的预测抖动变大，回访命中率下降。这几项是上线后最关键的收益来源


流程：
第一步，在无任何NPS调研标签的情况下，仅基于KQI数据生成树模型和神经网络的基础模型结构，
并利用孤立森林方法构造伪标签，进而训练出初始模型参数，由此获得功能可用的主体模型，此步骤对模型精度不作要求。

第二步，在获取有效数量的NPS调研结果后，对第一步生成的基础模型进行二次调优，使主体模型达到期望模型精度。

第三步，在运行态下，通过定时触发方式，将KQI数据输入至主体模型，并使模型输出的概率按业务需要予以保存。
得到预处理和预训练的模型后 输出集成  到transformer神经网络模型中，伪标签训练 之后进一步调优。

基于上述步骤，将按如下模块算法进行详细介绍，包括：

（1）孤立森林模块：用于构造伪标签，并基于此生成基础模型结构和初始参数；
（2）数据预处理模块：包括宽表转矩阵和非线性归一化，该过程为共用计算逻辑，
其输出为LightGBM树模型和Transformer神经网络的输入；
（3）特征提取模块：为LightGBM树模型的输入数据特征计算过程；
（4）LightGBM模块：为子模型之一，树模型；
（5）Transformer模块：为子模型之一，神经网络模型；
（6）输出集成模块：为子模型输出结果的集成，得到主体模型的最终输出结果。

数据存在的问题：
1/NPS分数与高维宽表KQI之间相关性弱，无法通过无监督方式挖掘其复杂映射关系，需要根据真实标签数据学习其分布特征。
1、客户无法及时提供NPS调研样本；
2、客户希望该AI应用一经安装就可进行日常推理任务；
3、由于不同地区的KQI数据分布差异较大，无法发布通用基础模型；

贬损结果的用户群体，其网络质量应有别于正常指标分布，呈现离群性。
孤立森林模型，通过离群值检测能力来构造无调研报告数据的伪标签（pseudo label），进而借助伪标签训练得到基础模型。



孤立森林（isolation forest）[1]为一种无监督学习的树模型方法，常用于异常值或离群点检测，
该模型的学习过程是通过对训练集的递归分隔来建立的，直到所有的样本被孤立，或者树达到了指定的高度则停止训练，
最终每个叶子节点记录了各项特征的分隔值。
模型判断样本数据为异常值是基于样本落入叶子节点所经过的路径长度的期望值，该路径越短则异常分数越大

将对相同的训练集进行推理，从而生成正负标签，其中输出值为1/0表示离群/正常样本，视为正/负样本。
数据预处理

均值方差归一化
和非线性变换
sigmoid非线性函数。

1、函数因变量范围限制在(0,1)之间，可进一步对数据范围进行规范化；
2、两端具有抑制作用，可有效处理异常离群值；
3、以0值为中心，导数小于1且向两端逐渐趋于0，可在保持相对大小的条件下，将数据向两端方向拉伸，有利于分类。

为了从中筛选真正有显著增益的算子，本方案使用前向特征选择思路，即依次逐步增加特征的方法，通过构建LightGBM树模型，
以5折交叉验证的平均AUC作为筛选条件，根据平均AUC排序结果，人工挑选合适的特征提取方法。

Light GBM 树模型  弱学习器组合 
超参数

Transformer神经网络模型
分别为线性层、多头多层自注意力模块、门控层和输出层。

线性层
输入数据的时间维度进行处理，使模型学习KQI再时间维度上的交叉特征。本层参数量（权重与偏移量）为30*30+30=930

位置编码
提供时间节点的位置信息，使模型学习KQI与不同时间节点的分布关联。位置编码的位置序号是三角函数生成的结果。偶数为
正弦函数，奇数为余弦函数

类别嵌入层 
结合图像位置与内容的高维特征表示。任何一个子快特征不能判断整体图像的类别。需要全局信息来表征图像的总体信息。
在（1）中的线性层之后，在时间维度前增加一个与KQI同维度的随机单位均匀分布初始化向量，作为类别嵌入，
用于学习总样本的特征信息，从而在后续的过程中参与特征的分类计算过程。

多头注意力层
token，作为注意力机制中的key/query/value来计算相似度，使模型学习携带时间序列信息的KQI之间的相关性。


取值    含义
optimizer     AdamWeightDecay
优化器，网络参数的更新策略，合理的优化器可以使网络避免进入局部最优点，且可以达到更好的模型精度和更快的训练速度。

learning_rate   0.01(训练)    0.002(调优)
学习率（也称步长），用于控制网络参数更新时的梯度的大小，合理的学习率可以防止模型参数的抖动，提升模型对样本的学习精度。

weight_decay   10-4
正则化项的权重系数，用于训练模型时对网络参数的施加L2正则化，防止模型过拟合。


（1）训练过程选择较大的batch_size和learning_rate以及较小的epoch，以实现大规模数据下的模型快速学习；
（2）调优过程选择较小的batch_size和learning_rate，实现微调基础模型参数，从而对真实样本准确学习，
同时选择较大的epoch，防止模型欠拟合，但需注意epoch不可过大，通过观察训练集和测试集上的损失值
是否差距较大来判断是否过拟合。



我认为“高级的快乐”是“沧海一声笑”的快乐，可以说是在了然，领悟了“极意”之后的快乐，这快乐是打破了守恒的快乐。
什么叫快乐守恒呢？就是“零和游戏”，总量不变的快乐，你获得了快乐，必然随之有人痛苦。我承认“为所欲为”的境界才是真的强。
