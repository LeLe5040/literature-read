
1. BN和dropout训练和测试的差异
Dropout：
训练时：假设保留率是 0.8，每轮都会随机关闭 20% 的神经元。
测试时：不再丢神经元，但要对输出进行缩放（乘 0.8），以匹配训练时的平均激活强度。

Batch Normalization：
训练时：每个小批次算自己的均值/方差，然后归一化。
测试时：用训练过程中记录下来的“全局平均统计量”（running mean/var），使得结果更稳定。

2. 在二分类任务中常用的 loss 函数和最后一层的激活函数分别是什么？为什么
在二分类任务中，常用的损失函数是 Binary Cross Entropy（BCE）或 BCEWithLogitsLoss，最后一层的激活函数通常是 Sigmoid。
如果使用 BCELoss，则最后一层要加 Sigmoid，用于将输出映射到概率空间 (0,1)。
如果使用 BCEWithLogitsLoss，则不加 Sigmoid，因为它内部已集成 sigmoid + BCE，更数值稳定，是工业界推荐做法。


3. word2vec用的哪一个，解释这个方法，损失函数是交叉熵吗，损失函数的公式？损失函数中负样本损失是一个还是多个

4.逻辑回归损失函数级梯度更新的工时
交叉熵
损失函数
二分类神经网络 含前馈和后馈

transformer的结构  里面的attention机制
多头注意力
注意力计算的复杂度  降低复杂度的办法 
transformer 的大致描述一下，讲一下前向流程？
讲一下bert, bert训练的损失函数是什么？
bert 的三种编码向量为什么可以直接相加？
分类模型一般用什么损失？为什么用交叉熵不用MSE？写一下交叉熵的公式？



梯度消失 梯度爆炸 梯度下降  原因 如何解决 
L1L2正则化
防止过拟合的办法
交叉熵和最大似然的关系 
损失函数有哪些  
常见的激活函数及优缺点 
各种归一化对的方法（layernorm，batchnorm）的却别
解释过拟合和欠拟合，如何结果的


softmax与二分类有什么特点 
二分类任务（ CTR / CVR )使用的 loss 和最后一层的激活函数是什么？为什么？写出这里的每个激活函数、 loss function
2．为什么二分类任务的预估值等于 label 的均值，前提：做一个二分类任务，输入的数据 x 都相同，其 label 存在不相同。
3．回归任务可以使用哪些 loss function ？最后一层分别用什么激活函数？写出这里的每个 loss function ，并关于预估值求导
随机森林  机器学习 
kmeans  损失函数 每个蔟的中心点事怎么选的 参数等等

手撕 layer normation
手撕大数相加 
手撕最大组数和
手撕 最短无序子数组
code题目:打家劫舍
手撕：计算岛屿数量
4. 手撕：最长递增子序列
滑动窗口的最大值
三数之和 

AUC的含义  两个物理意义 公式和时间复杂度  随机采样负样本对AUC有什么影响
softmax的真正底层原理，问的很细致（答得很浅）
讲一下XGB, 决策树如何选择特征（项目中涉及树模型
你了解哪些多模态模型？模态是怎么做融合的？


论文相关 
数据集介绍 如何构建的
论文的创新点和细节问答
论文的后续工作介绍 优化的点是啥
、
在我们的验证集上，Cross-Attention 和 多尺度窗口带来最明显收益：前者直接加强‘质量→体验’的对齐，显著提升了贬损召回；后者让模型避免只盯短期波动。独立 Encoder带来更稳定的泛化，时段/地域切换时鲁棒性更好。去掉归因 Loss后，主观样本的预测抖动变大，回访命中率下降。这几项是上线后最关键的收益来源


流程：
第一步，在无任何NPS调研标签的情况下，仅基于KQI数据生成树模型和神经网络的基础模型结构，
并利用孤立森林方法构造伪标签，进而训练出初始模型参数，由此获得功能可用的主体模型，此步骤对模型精度不作要求。

第二步，在获取有效数量的NPS调研结果后，对第一步生成的基础模型进行二次调优，使主体模型达到期望模型精度。

第三步，在运行态下，通过定时触发方式，将KQI数据输入至主体模型，并使模型输出的概率按业务需要予以保存。
得到预处理和预训练的模型后 输出集成  到transformer神经网络模型中，伪标签训练 之后进一步调优。

基于上述步骤，将按如下模块算法进行详细介绍，包括：

（1）孤立森林模块：用于构造伪标签，并基于此生成基础模型结构和初始参数；
（2）数据预处理模块：包括宽表转矩阵和非线性归一化，该过程为共用计算逻辑，
其输出为LightGBM树模型和Transformer神经网络的输入；
（3）特征提取模块：为LightGBM树模型的输入数据特征计算过程；
（4）LightGBM模块：为子模型之一，树模型；
（5）Transformer模块：为子模型之一，神经网络模型；
（6）输出集成模块：为子模型输出结果的集成，得到主体模型的最终输出结果。

数据存在的问题：
1/NPS分数与高维宽表KQI之间相关性弱，无法通过无监督方式挖掘其复杂映射关系，需要根据真实标签数据学习其分布特征。
1、客户无法及时提供NPS调研样本；
2、客户希望该AI应用一经安装就可进行日常推理任务；
3、由于不同地区的KQI数据分布差异较大，无法发布通用基础模型；

贬损结果的用户群体，其网络质量应有别于正常指标分布，呈现离群性。
孤立森林模型，通过离群值检测能力来构造无调研报告数据的伪标签（pseudo label），进而借助伪标签训练得到基础模型。



孤立森林（isolation forest）[1]为一种无监督学习的树模型方法，常用于异常值或离群点检测，
该模型的学习过程是通过对训练集的递归分隔来建立的，直到所有的样本被孤立，或者树达到了指定的高度则停止训练，
最终每个叶子节点记录了各项特征的分隔值。
模型判断样本数据为异常值是基于样本落入叶子节点所经过的路径长度的期望值，该路径越短则异常分数越大







我认为“高级的快乐”是“沧海一声笑”的快乐，可以说是在了然，领悟了“极意”之后的快乐，这快乐是打破了守恒的快乐。
什么叫快乐守恒呢？就是“零和游戏”，总量不变的快乐，你获得了快乐，必然随之有人痛苦。我承认“为所欲为”的境界才是真的强。
